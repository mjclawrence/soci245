lat = center_lat,
lon = center_lng
) %>%
select(date, lat, lon) %>%
distinct() %>%
getSunlightTimes(
data = .,
keep = c("sunset", "dusk"),
tz = tz
) %>%
mutate_at(vars("sunset", "dusk"), ~format(., "%H:%M:%S")) %>%
mutate(
sunset_minute = time_to_minute(sunset),
dusk_minute = time_to_minute(dusk),
date = ymd(str_sub(date, 1, 10))
) %>%
select(date, sunset, dusk, ends_with("minute"))
sunset_times %>%
filter(dusk == min(dusk) | dusk == max(dusk))
vod_stops <- readRDS(gzcon(url("https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project_vod_stops.rds")))
vod_stops <- readRDS(gzcon(url("https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project_vod_stops.rds")))
data_link <- "https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project_vod_stops.rds" # where the data is posted
load(url(data_link))
load(url(data_link))
vod_stops <- readRDS(gzcon(url("https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project_vod_stops.rds")))
View(vod_stops)
## Libraries to include
library(tidyverse)
library(lubridate)
# For Veil of Darkness
library(lutz)
library(suncalc)
library(splines)
## Load the data
# Replace the path below with the path to where your data lives
stops <- readRDS(gzcon(url("https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project.rds")))
# Additional data and fixed values we'll be using
population_2017 <- tibble(
subject_race = c(
"asian/pacific islander", "black", "hispanic", "other/unknown","white"
),
num_people = c(110864, 648846, 221777, 39858, 548312)
) %>%
mutate(subject_race = as.factor(subject_race))
center_lat <- 39.9525839
center_lng <- -75.1652215
colnames(stops)
nrow(stops)
min(stops$date)
max(stops$date)
stops <- filter(stops, year(date) < 2018)
# NOTE that this is equivalent to `nrow(stops)`
stops %>% nrow()
stops %>%
select(type) %>%
head(10) # this allows us to look at just the first 10 rows
stops <- stops %>% filter(type == "vehicular")
stops %>% nrow()
stops %>%
mutate(year = year(date)) %>%
count(year)
stops %>%
count(subject_race)
# This method builds off of using `count` as above
stops %>%
count(subject_race) %>%
mutate(prop = n / sum(n))
# This method uses the group_by/summarize paradigm
stops %>%
group_by(subject_race) %>%
summarize(
n = n(),
prop = n / nrow(.)
)
stops %>%
# notice that you can also mutate `date` *within* the count funciton
count(year = year(date), subject_race)
stops %>%
count(year = year(date), subject_race) %>%
ggplot(aes(x = year, y = n, color = subject_race)) +
geom_point() +
geom_line()
stops <- stops %>% filter(year(date) == 2017)
population_2017 %>%
mutate(prop = num_people / sum(num_people))
stops %>%
count(subject_race) %>%
left_join(
population_2017,
by = "subject_race"
) %>%
mutate(stop_rate = n / num_people)
stops %>%
group_by(subject_race) %>%
summarize(
search_rate = mean(search_conducted, na.rm = T),
frisk_rate = mean(frisk_performed, na.rm = T)
)
stops %>%
filter(search_conducted) %>%
group_by(subject_race) %>%
summarize(
hit_rate = mean(contraband_found, na.rm = T)
)
hit_rates <-
stops %>%
filter(search_conducted) %>%
group_by(subject_race, district) %>%
summarize(hit_rate = mean(contraband_found, na.rm = T))
hit_rates
# Reshape table to show hit rates of minorities vs white drivers
hit_rates <-
hit_rates %>%
filter(subject_race %in% c("black", "white", "hispanic")) %>%
spread(subject_race, hit_rate, fill = 0) %>%
rename(white_hit_rate = white) %>%
gather(minority_race, minority_hit_rate, c(black, hispanic)) %>%
arrange(district)
hit_rates
# We'll use this just to make our axes' limits nice and even
max_hit_rate <-
hit_rates %>%
select(ends_with("hit_rate")) %>%
max()
hit_rates %>%
ggplot(aes(
x = white_hit_rate,
y = minority_hit_rate
)) +
geom_point() +
# This sets a diagonal reference line (line of equal hit rates)
geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
# These next few lines just make the axes pretty and even
scale_x_continuous("White hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
scale_y_continuous("Minority hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
# This makes sure that 1% on the x-axis is the same as 1% on the y-axis
coord_fixed() +
# This allows us to compare black v. white and Hispanic v. white side by
# side, in panels
facet_grid(. ~ minority_race)
# Depending on your version of ggplot2, you may be able to use the syntax
# below (the newer ggplot2 syntax)---which is clearer, in my opinion.
# But older versions of ggplot2 will only accept the above syntax
# facet_grid(cols = vars(minority_race))
# Get corresponding number of searches (to size points).
# Again, for each district we want to know the number of white+black searches
# and white+Hispanic searches. This requires the same spreading and gathering
# as our previous data-munging.
search_counts <-
stops %>%
filter(
search_conducted,
subject_race %in% c("black", "white", "hispanic")
) %>%
count(district, subject_race) %>%
spread(subject_race, n, fill = 0) %>%
rename(num_white_searches = white) %>%
gather(minority_race, num_minority_searches, c(black, hispanic)) %>%
mutate(num_searches = num_minority_searches + num_white_searches) %>%
select(district, minority_race, num_searches)
hit_rates %>%
left_join(
search_counts,
by = c("district", "minority_race")
) %>%
ggplot(aes(
x = white_hit_rate,
y = minority_hit_rate
)) +
geom_point(aes(size = num_searches), pch = 21) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
scale_x_continuous("White hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
scale_y_continuous("Minority hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
coord_fixed() +
facet_grid(. ~ minority_race)
# same facet_grid syntax issue here as before!
# facet_grid(cols = vars(minority_race))
hit_rates %>%
filter(white_hit_rate == min(white_hit_rate))
# Hint: Note that districts in our dataset are encoded as characters,
# so 77 is actually "77"
stops %>%
filter(district == "77") %>%
count(location, sort = T)
# Remember: Districts in our dataset are encoded as characters,
# so 77 is actually "77"
stops %>%
filter(search_conducted, district != "77") %>%
group_by(subject_race) %>%
summarize(
hit_rate = mean(contraband_found, na.rm = T)
)
# Get timezone for Philly
tz <- lutz::tz_lookup_coords(center_lat, center_lng, warn = F)
# Helper function
time_to_minute <- function(time) {
hour(hms(time)) * 60 + minute(hms(time))
}
# Compute sunset time for each date in our dataset
sunset_times <-
stops %>%
mutate(
lat = center_lat,
lon = center_lng
) %>%
select(date, lat, lon) %>%
distinct() %>%
getSunlightTimes(
data = .,
keep = c("sunset", "dusk"),
tz = tz
) %>%
mutate_at(vars("sunset", "dusk"), ~format(., "%H:%M:%S")) %>%
mutate(
sunset_minute = time_to_minute(sunset),
dusk_minute = time_to_minute(dusk),
date = ymd(str_sub(date, 1, 10))
) %>%
select(date, sunset, dusk, ends_with("minute"))
sunset_times %>%
filter(dusk == min(dusk) | dusk == max(dusk))
vod_stops <- readRDS(gzcon(url("https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project_vod_stops.rds")))
vod_stops <-
stops %>%
left_join(
sunset_times,
by = "date"
) %>%
mutate(
minute = time_to_minute(time),
minutes_after_dark = minute - dusk_minute,
is_dark = minute > dusk_minute,
min_dusk_minute = min(dusk_minute),
max_dusk_minute = max(dusk_minute),
is_black = subject_race == "black"
) %>%
filter(
# Filter to get only the intertwilight period
minute >= min_dusk_minute,
minute <= max_dusk_minute,
# Remove ambigous period between sunset and dusk
!(minute > sunset_minute & minute < dusk_minute),
# Compare only white and black drivers
subject_race %in% c("black", "white")
)
save(vod_stops, file = "/users/lawrence/desktop/open_policing_project_vod_stops.Rda")
vod_stops <- "https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project_vod_stops.Rda" # where the data is posted
load(url(vod_stops))
vod_stops %>% nrow()
vod_stops %>%
filter(time > hm("18:30"), time < hm("18:45")) %>%
group_by(is_dark) %>%
summarize(prop_black = mean(is_black))
mod1 <- glm(
is_black ~ is_dark + splines::ns(minute, df = 6),
family = binomial,
data = vod_stops
)
summary(mod1)$coefficients["is_darkTRUE", c("Estimate", "Std. Error")]
mod2 <- glm(
is_black ~ is_dark + splines::ns(minute, df = 6) + as.factor(district),
family = binomial,
data = vod_stops
)
summary(mod2)$coefficients["is_darkTRUE", c("Estimate", "Std. Error")]
## Libraries to include
library(tidyverse)
library(lubridate)
# For Veil of Darkness
library(lutz)
library(suncalc)
library(splines)
## Load the data
# Replace the path below with the path to where your data lives
stops <- readRDS(gzcon(url("https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project.rds")))
# Additional data and fixed values we'll be using
population_2017 <- tibble(
subject_race = c(
"asian/pacific islander", "black", "hispanic", "other/unknown","white"
),
num_people = c(110864, 648846, 221777, 39858, 548312)
) %>%
mutate(subject_race = as.factor(subject_race))
center_lat <- 39.9525839
center_lng <- -75.1652215
colnames(stops)
nrow(stops)
min(stops$date)
max(stops$date)
stops <- filter(stops, year(date) < 2018)
# NOTE that this is equivalent to `nrow(stops)`
stops %>% nrow()
stops %>%
select(type) %>%
head(10) # this allows us to look at just the first 10 rows
stops <- stops %>% filter(type == "vehicular")
stops %>% nrow()
stops %>%
mutate(year = year(date)) %>%
count(year)
stops %>%
count(subject_race)
# This method builds off of using `count` as above
stops %>%
count(subject_race) %>%
mutate(prop = n / sum(n))
# This method uses the group_by/summarize paradigm
stops %>%
group_by(subject_race) %>%
summarize(
n = n(),
prop = n / nrow(.)
)
stops %>%
# notice that you can also mutate `date` *within* the count funciton
count(year = year(date), subject_race)
stops %>%
count(year = year(date), subject_race) %>%
ggplot(aes(x = year, y = n, color = subject_race)) +
geom_point() +
geom_line()
stops <- stops %>% filter(year(date) == 2017)
population_2017 %>%
mutate(prop = num_people / sum(num_people))
stops %>%
count(subject_race) %>%
left_join(
population_2017,
by = "subject_race"
) %>%
mutate(stop_rate = n / num_people)
stops %>%
group_by(subject_race) %>%
summarize(
search_rate = mean(search_conducted, na.rm = T),
frisk_rate = mean(frisk_performed, na.rm = T)
)
stops %>%
filter(search_conducted) %>%
group_by(subject_race) %>%
summarize(
hit_rate = mean(contraband_found, na.rm = T)
)
hit_rates <-
stops %>%
filter(search_conducted) %>%
group_by(subject_race, district) %>%
summarize(hit_rate = mean(contraband_found, na.rm = T))
hit_rates
# Reshape table to show hit rates of minorities vs white drivers
hit_rates <-
hit_rates %>%
filter(subject_race %in% c("black", "white", "hispanic")) %>%
spread(subject_race, hit_rate, fill = 0) %>%
rename(white_hit_rate = white) %>%
gather(minority_race, minority_hit_rate, c(black, hispanic)) %>%
arrange(district)
hit_rates
# We'll use this just to make our axes' limits nice and even
max_hit_rate <-
hit_rates %>%
select(ends_with("hit_rate")) %>%
max()
hit_rates %>%
ggplot(aes(
x = white_hit_rate,
y = minority_hit_rate
)) +
geom_point() +
# This sets a diagonal reference line (line of equal hit rates)
geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
# These next few lines just make the axes pretty and even
scale_x_continuous("White hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
scale_y_continuous("Minority hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
# This makes sure that 1% on the x-axis is the same as 1% on the y-axis
coord_fixed() +
# This allows us to compare black v. white and Hispanic v. white side by
# side, in panels
facet_grid(. ~ minority_race)
# Depending on your version of ggplot2, you may be able to use the syntax
# below (the newer ggplot2 syntax)---which is clearer, in my opinion.
# But older versions of ggplot2 will only accept the above syntax
# facet_grid(cols = vars(minority_race))
# Get corresponding number of searches (to size points).
# Again, for each district we want to know the number of white+black searches
# and white+Hispanic searches. This requires the same spreading and gathering
# as our previous data-munging.
search_counts <-
stops %>%
filter(
search_conducted,
subject_race %in% c("black", "white", "hispanic")
) %>%
count(district, subject_race) %>%
spread(subject_race, n, fill = 0) %>%
rename(num_white_searches = white) %>%
gather(minority_race, num_minority_searches, c(black, hispanic)) %>%
mutate(num_searches = num_minority_searches + num_white_searches) %>%
select(district, minority_race, num_searches)
hit_rates %>%
left_join(
search_counts,
by = c("district", "minority_race")
) %>%
ggplot(aes(
x = white_hit_rate,
y = minority_hit_rate
)) +
geom_point(aes(size = num_searches), pch = 21) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
scale_x_continuous("White hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
scale_y_continuous("Minority hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
coord_fixed() +
facet_grid(. ~ minority_race)
# same facet_grid syntax issue here as before!
# facet_grid(cols = vars(minority_race))
hit_rates %>%
filter(white_hit_rate == min(white_hit_rate))
# Hint: Note that districts in our dataset are encoded as characters,
# so 77 is actually "77"
stops %>%
filter(district == "77") %>%
count(location, sort = T)
# Remember: Districts in our dataset are encoded as characters,
# so 77 is actually "77"
stops %>%
filter(search_conducted, district != "77") %>%
group_by(subject_race) %>%
summarize(
hit_rate = mean(contraband_found, na.rm = T)
)
sunset_times %>%
filter(dusk == min(dusk) | dusk == max(dusk))
vod_stops <- "https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project_vod_stops.Rda" # where the data is posted
load(url(vod_stops))
vod_stops %>% nrow()
vod_stops %>%
filter(time > hm("18:30"), time < hm("18:45")) %>%
group_by(is_dark) %>%
summarize(prop_black = mean(is_black))
mod1 <- glm(
is_black ~ is_dark + splines::ns(minute, df = 6),
family = binomial,
data = vod_stops
)
summary(mod1)$coefficients["is_darkTRUE", c("Estimate", "Std. Error")]
rm(list = ls()) # Erase all the data in the memory
vod_stops <- "https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project_vod_stops.Rda" # where the data is posted
load(url(vod_stops))
vod_stops %>% nrow()
vod_stops %>%
filter(time > hm("18:30"), time < hm("18:45")) %>%
group_by(is_dark) %>%
summarize(prop_black = mean(is_black))
mod1 <- glm(
is_black ~ is_dark + splines::ns(minute, df = 6),
family = binomial,
data = vod_stops
)
summary(mod1)$coefficients["is_darkTRUE", c("Estimate", "Std. Error")]
mod2 <- glm(
is_black ~ is_dark + splines::ns(minute, df = 6) + as.factor(district),
family = binomial,
data = vod_stops
)
summary(mod2)$coefficients["is_darkTRUE", c("Estimate", "Std. Error")]
gc()
## Libraries to include
library(tidyverse)
library(lubridate)
# For Veil of Darkness
library(lutz)
library(suncalc)
library(splines)
## Load the data
# Replace the path below with the path to where your data lives
stops <- readRDS(gzcon(url("https://raw.githubusercontent.com/mjclawrence/soci245/s20/data/open_policing_project.rds")))
# Additional data and fixed values we'll be using
population_2017 <- tibble(
subject_race = c(
"asian/pacific islander", "black", "hispanic", "other/unknown","white"
),
num_people = c(110864, 648846, 221777, 39858, 548312)
) %>%
mutate(subject_race = as.factor(subject_race))
center_lat <- 39.9525839
center_lng <- -75.1652215
gc()
